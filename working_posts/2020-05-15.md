# 정규성, 다중공선성, 등분산성... 머신러닝의 통계적 가정 확인하고 개선하기

### 참고
- 머신러닝 모델들의 장단점 정리: https://www.dummies.com/programming/big-data/data-science/machine-learning-dummies-cheat-sheet/ 
- 머신러닝 모델들의 통계적 가정 정리: https://towardsdatascience.com/all-the-annoying-assumptions-31b55df246c3
- 통계적 가정 검토하기: https://towardsdatascience.com/the-importance-of-analyzing-model-assumptions-in-machine-learning-a1ab09fb5e76
- 및 scikit-learn 공식문서
- 다중공선성은 왜 문제인가: https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/
- normality test with python: https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/



## 까다로운 통계적 가정 꼭 검토해야 할까?
정답은 '그렇다'이다. 많은 머신러닝 모델들은 통계 모델에 기반한다. 제일 처음 배우는 선형회귀 모델만 해도 선형성, 등분산성 등 여섯가지 조건을 요구한다. 원론적으로 말하자면 이 조건을 모두 만족해야 모델을 적용하는 것이 옳다. 모델 내부에서는 해당 조건이 만족되었다고 가정하고 연산을 하기 때문이다. 조건에 맞지 않는 데이터를 입력하면 정확한 학습과 예측이 불가능한 것은 당연지사다. 

그런데 그 조건들이라는 게 정말 까다롭다! 실제 프로젝트에 들어가서 이런 가정을 모두 만족하는 데이터를 맞닥뜨리는 경우는 운이 좋거나 돈이 많거나 둘 중에 하나다. 운도 없고 돈도 없는 나는 단 한 번도 완벽한 데이터를 만나본 적은 없다. 그렇다고 모델 만들기를 포기할 것인가? 전에 한 프로젝트에서 통계 모델을 적용할 때 무작위성 조건 때문에 고민하던 나에게 통계학과 교수님은 아래처럼 답변을 주셨다.

"무작위 표본이라는 전제는 사실 학부생 뿐 만 아니라 자본력이 없으면 하기 힘든 것 같습니다. 현실적으로는 통계 분석 전에 정규분포 분위수 대조도를 통해 정규모집단을 전제할 수 있는지 확인하는 것이 최대일 것 같습니다. 만약 전제할 수 없다해도 우선은 통계처리를 해서 결과를 드리고 정규모집단 전제 성립 여부도 함께 드리겠습니다. 수업이나 교수님이 요구하는 수준이나 연구자 개인의 기준에 따라, 폐기할지 말지는 선택에 남겨두겠습니다."

이를 머신러닝에 적용해보면 통계적인 가정은 모두 확인하되, 얼마나 엄격하게 적용할 것인가는 상황과 목적에 따라 유동적으로 선택하는 것이 좋다는 것이다. 통계적 가설을 확인하는 과정에서 데이터의 분포와 특이점에 대해서 이해하게 되고, 특정 모델에 적합하도록 추가적으로 데이터를 정제하거나 데이터의 분포에 적합한 다른 모델을 찾아볼 수 있다. 그럼에도 여전히 만족하지 못하는 가정이 있다면 그대로 진행하되 같이 보고해야한다는 교훈이다. 그러므로 앞으로 살펴볼 주요 모델들의 통계적 가정은 반드시 살펴보기는 하되 분석의 목적에 맞게 엄격하게 적용할지 느슨하게 적용할지 선택하면 된다. 

교수님의 답변을 통해(+학부 때 수학을 전공하고 컴퓨터공학부 석박과정에 진학하여 데이터 사이언스를 공부하고 있는 지인에게 문의하여) 내가 나름대로 세워본 프로세스는 다음과 같다.
1. 기본적인 데이터 분포를 확인한다
2. 적용할 모델을 추린 후 추가적으로 더 필요한 통계적 가정을 검토한다
3. 모델이 제대로 적용됐는지 summary나 residual plot을 통해 진단한다
4. 가정의 성립 여부를 포함하여 결론을 낸다.

첫번째 단계, 기본적인 데이터 분포를 확인할 때는 주요한 통계 개념이 분석에 가이드라인이 되어 줄 수 있다. 여기에서는 그 중에서도 반복적으로 등장하는 다중공선성과 정규성에 대해 그 개념과 모델에서의 의미 및 해결 방안을 알아보겠다.

## 다중공선성(Multicollinearity)
독립변수는 '독립'적이어야 한다! 그런데 데이터셋에서 feature에 해당하는 이 독립변수들이 서로 종속관계에 있을 때 다중공선성이 존재한다고 말한다. feature에 불필요한 중복이 있다는 뜻으로 이해할 수 있다. 아래처럼 correlation matrix를 통해 쉽게 확인할 수 있다.
```python
corrMatrix = df.corr()
sns.heatmap(corrMatrix, annot=True)
```
이 [글](https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec)에서는 아래 사진처럼 correlation matirx를 보다 효과적으로 시각화하는 방법을 제시하고 있다

### 왜 문제인가
이는 regression 모델들에서 특히 치명적이다. 회귀 문제에서는 각 x 변수가 한 단위 변화할 때 y 변수가 어떻게 변화하는 지 알아내고자 한다. 이 때, 나머지 변수들은 고정되어 있는 상수로 취급하는데, 종속 관계에 있는 다른 변수는 고정되는 것이 불가능하다. 결과적으로 회귀 문제에서 가중치를 제대로 판별할 수 없게 된다.
> 만약 종속변수에 대한 독립변수의 영향력을 평가하기 위함이 아니라, 종속변수를 정확하게 예측하는 것이 목표라면 다중공선성은 크게 문제되지 않는다. 다중공선성은 개별 변수의 p-value와 상관계수에 영향을 미칠 뿐이다. 같은 원리로 독립 변수 중에서도 해당 실험의 분석 대상이 아닌 통제 변수들이 섞여있다면, 이들 통제 변수 간의 다중공선성도 크게 문제되지 않는다(통제 변수의 상관계수에 크게 의미를 부여할 것이 아니므로).  
또한 다중공선성은 모델의 안정성을 저해한다. 데이터 상의 작은 변화에도 상관계수들의 크기가 뒤바뀌는 것이다. 이는 곧 데이터의 이상치와 랜덤한 노이즈에도 민감하다는 뜻으로, 다중공선성을 제거하지 않으면 오버피팅의 위험도 커진다.
### 해결책
실제 생활에서는 변수들이 어느 정도 함께 변화하기 때문에, 다중공선성을 완전히 제거하여 100% 독립적인 변수들만 남긴다는 것은 현실적으로 어렵다. correlation matrix에서 상관계수가 기준치보다 높게 나오는 칼럼들 중 하나를 삭제하는 방법을 사용할 수도 있다. 이 때에는 분석의 목표를 고려하여 논리적으로 더 적합한 변수를 선택하거나 종속변수와의 상관계수가 더 높은 변수를 선택할 수 있다. 아니면 상관성이 높게 나오는 변수들을 합쳐 PCA나 partial least squares regression 등의 방법으로 새로운 변수를 만드는 것도 방법이다.
### 적용 모델
- 회귀 모델: 앞서 말했다시피 회귀 모델은 변수들이 독립적인 상태를 가정한다. 이는 linear 및 polynomial regression은 물론 logistic regression에도 마찬가지다.
- Naive Bayes Classifier: ?? 아닌듯
- Decision Tree: 의사결정나무와 Random Forest를 적용할 때 다중공선성을 검토해야 하는 지에 대해서는 의견이 갈린다.

Q. correlation != dependence
단순히 correlation이 없다는 것을 확인하는 것으로는 변수들이 독립적이라고 단정할 수 없다! https://www.quora.com/Why-does-Naive-Bayes-perform-poorly-when-there-is-multicollinearity-in-the-data

## 정규성
Parametric model들은 기본적으로 데이터가 특정한 분포(정규분포, 베르누이 분포, 베타 분포 등)에서 도출되었다고 가정한다. 그리고 키나 몸무게처럼 실생활의 많은 변수들이 종 모양의 정규분포를 따르기 때문에 많은 알고리즘이 정규분포를 전제한다. Linear Regression도 그 중 하나다. 
> 사실 정규성을 확인하는 것은 모델을 학습할 때 뿐만 아니라, 모델 선택 단계에서 모델의 결과를 평가할 때도 사용되며, regression에서는 모델의 학습 결과로 남은 잔차 또한 정규 분포를 띄어야 한다.




그 와중에 철두철미한 사람들은 통계적 normality test를 개발해서 검정통계량과 p value로 '아 이 정도면 정규분포지'하는 정신승리를 방지했다. 왜곡된 행복회로를 방지하고 싶은 사람들을 위한 [statistical normality test](https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/)

### 왜 문제인가

### 해결책
가장 쉬운 해결책은 linear regression을 포기해버리는 것이다. KNN 같은 비모수적 모델들을 보면 얼마나 간단한가! 파라미터의 수가 유동적이거나 데이터를 학습함에따라 증가하는 비모수적 모델들은 모수적 모델들보다 데이터에 대한 가정이 적다. 하지만 계산이 느리고 검정력(power)이 떨어져 같은 수준의 신뢰 수준에 도달하기 위해 더 많은 샘플을 필요로 한다.
다른 방법으로는 가지고 있는 데이터를 잘 변환해서 모델이 요구하는 정규분포의 종모양을 최대한 만족시키려 노오력하는 것이다.

많은  이를 만족하지 않으면 nonparametric model을 활용하면 된다. 하지만 비모수적 모델들은 보통 power가 떨어진다. 즉, 같은 수준의 신뢰 수준에 도달하기 위해 더 많은 샘플이 필요하다. 그러므로 가지고 있는 데이터를 잘 변환해서(normalization, 로그 변환 등) 사용하고자 하는 모델의 가정을 만족하려고 노력해보자 You want to normalize data to better fit the distribution your model was developed for
- 스택오버더플로우의 한 글(https://stackoverflow.com/questions/56777257/why-normality-is-considered-as-an-important-assumption-for-dependent-and-indepen) 에서는 독립변수는 필요없고 종속변수만 정규성 가정을 만족하면 된다는데?




## 주요 모델들의 통계적 가정
### Linear Regression
선형 회귀는 예측하고자하는 타겟 변수의 실제 값과 예측 값의 차이 제곱합(Residual Sum of Squares 혹은 Sum of Squared Residual)이 최고학 되도록 선형 모델의 계수를 학습한다. 
> RSS로 가중치를 학습하는 방식을 Ordinary Least Squares 라고 부른다. 즉, scipy의 `linear_model.LinearRegression`은 통계 도메인에서 활용하는 OLS에 `predict` 등 머신러닝에 필요한 메소드를 더한 것이라고 보면된다. 하지만 OLS는 non linear modeling에도 쓰이는 최적화 방법이므로 주의할 것!
### 가정
1. 선형성: scatter plot를 그리거나 피어슨 상관계수를 계산해 보며 대략적으로 선형관계가 있다면 선형 회귀 모델의 의미 있을 것이라고 추측할 수 있다. 비선형적인 모양, 예를 들면 U자 모양을 띈다면 선형 회귀로는 해당 데이터 분포를 포착하기 어렵다. 독립변수가 2개 이상이라면? 각각 종속변수와의 선형 관계를 확인한다.
2. 등분산성:
3. 독립성: 
4. 정규성: Q-Q 플랏을 통해
5. 오차항의 정규성
6. 
### 강점
1. 이해하기 간단하다. 이는 곧 남들한테 설명할 때에도 간단하다는 뜻. 도출된 식에서 곧바로 독립변수가 한 단위 변화할 때 종속변수는 얼만큼 변화한다라는 관찰을 얻을 수 있다.
2. 오버피팅이 드물다
3. L1 혹은 L2 정규화를 활용해서 효율적인 feature selection이 가능하다
4. 학습이 빠르다
5. stochastic version을 통해 큰 데이터에도 적용할 수 있다.
### 단점
1. 비선형적인 관계를 포착하기 어렵다
2. 이상치에 민감하다



